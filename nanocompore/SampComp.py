# -*- coding: utf-8 -*-

#~~~~~~~~~~~~~~IMPORTS~~~~~~~~~~~~~~#
# Std lib
import collections, os, traceback, datetime
import multiprocessing as mp

# Third party
from loguru import logger
import yaml
#from tqdm import tqdm
import numpy as np
from Bio import SeqIO

# Local package
from nanocompore.common import *
import nanocompore.SampCompResultsmanager as SampCompResultsmanager
import nanocompore.TranscriptObject as TranscriptObject
import nanocompore.TxComp as TxComp

import nanocompore as pkg


# Disable multithreading for MKL and openBlas
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["MKL_THREADING_LAYER"] = "sequential"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ['OPENBLAS_NUM_THREADS'] = '1'

#~~~~~~~~~~~~~~MAIN CLASS~~~~~~~~~~~~~~#
class SampComp(object):
    """ Init analysis and check args"""

    #~~~~~~~~~~~~~~FUNDAMENTAL METHODS~~~~~~~~~~~~~~#

    def __init__(self,
        reference_samples:dict,
        test_samples:dict,
        fasta_fn:str,
        bed_fn:str = None,
        outpath:str = "results",
        outprefix:str = "out_",
        overwrite:bool = False,
        #whitelist:Whitelist = None,
        comparison_methods:list = ["GMM", "KS"],
        logit:bool = True,
        anova:bool = False,
        allow_warnings:bool = False,
        sequence_context:int = 0,
        sequence_context_weights:str = "uniform",
        min_coverage:int = 30,
        min_ref_length:int = 100,
        downsample_high_coverage:int = 5000,
        max_invalid_kmers_freq:float = 0.1,
        select_ref_id:list = [],
        exclude_ref_id:list = [],
        nthreads:int = 2,
        progress:bool = False):

        """
        Initialise a `SampComp` object and generates a white list of references with sufficient coverage for subsequent analysis.
        The retuned object can then be called to start the analysis.
        * eventalign_fn_dict
            Multilevel dictionnary indicating the condition_label, sample_label and file name of the eventalign_collapse output.
            2 conditions are expected and at least 2 sample replicates per condition are highly recommended.
            One can also pass YAML file describing the samples instead.
            Example `d = {"S1": {"R1":"path1.tsv", "R2":"path2.tsv"}, "S2": {"R1":"path3.tsv", "R2":"path4.tsv"}}`
        * outpath
            Path to the output folder.
        * outprefix
            text outprefix for all the files generated by the function.
        * overwrite
            If the output directory already exists, the standard behaviour is to raise an error to prevent overwriting existing data
            This option ignore the error and overwrite data if they have the same outpath and outprefix.
        * fasta_fn
            Path to a fasta file corresponding to the reference used for read alignment.
        * bed_fn
            Path to a BED file containing the annotation of the transcriptome used as reference when mapping.
        * whitelist
            Whitelist object previously generated with nanocompore Whitelist. If not given, will be automatically generated.
        * comparison_methods
            Statistical method to compare the 2 samples (mann_whitney or MW, kolmogorov_smirnov or KS, t_test or TT, gaussian_mixture_model or GMM).
            This can be a list or a comma separated string. {MW,KS,TT,GMM}
        * logit
            Force logistic regression even if we have less than 2 replicates in any condition.
        * allow_warnings
            If True runtime warnings during the ANOVA tests don't raise an error.
        * sequence_context
            Extend statistical analysis to contigous adjacent base if available.
        * sequence_context_weights
            type of weights to used for combining p-values. {uniform,harmonic}
        * min_coverage
            minimal read coverage required in all sample.
        * min_ref_length
            minimal length of a reference transcript to be considered in the analysis
        * downsample_high_coverage
            For reference with higher coverage, downsample by randomly selecting reads.
        * max_invalid_kmers_freq
            maximum frequency of NNNNN, mismatching and missing kmers in reads.
        * select_ref_id
            if given, only reference ids in the list will be selected for the analysis.
        * exclude_ref_id
            if given, refid in the list will be excluded from the analysis.
        * nthreads
            Number of threads (two are used for reading and writing, all the others for parallel processing).
        * progress
            Display a progress bar during execution
        """
        logger.info("Checking and initialising SampComp")

        # Save init options in dict for later
        log_init_state(loc=locals())


        # Check if fasta and bed files exist
        if not access_file(fasta_fn):
            raise NanocomporeError("{} is not a valid FASTA file".format(fasta_fn))
        if bed_fn and not access_file(bed_fn):
            raise NanocomporeError("{} is not a valid BED file".format(bed_fn))

        # Check threads number
        if nthreads < 2:
            raise NanocomporeError("The minimum number of threads is 2")

        # Parse comparison methods
        if comparison_methods:
            if type(comparison_methods) == str:
                comparison_methods = comparison_methods.split(",")
            for i, method in enumerate(comparison_methods):
                method = method.upper()
                if method in ["MANN_WHITNEY", "MW"]:
                    comparison_methods[i]="MW"
                elif method in ["KOLMOGOROV_SMIRNOV", "KS"]:
                    comparison_methods[i]="KS"
                elif method in ["T_TEST", "TT"]:
                    comparison_methods[i]="TT"
                elif method in ["GAUSSIAN_MIXTURE_MODEL", "GMM"]:
                    comparison_methods[i]="GMM"
                else:
                    raise NanocomporeError(f"Invalid comparison method {method}")


        # Set private args

        #set the output prefix
        if outprefix and outprefix.endswith('_'):
            self._outprefix = outprefix
        elif outprefix and not outprefix.endswith('_'):
            self._outprefix = f"{outprefix}_"
        else:
            self._outprefix = "out_"
        
        #if no outpath was provided, make it the current working directory
        if outpath:
            self._outpath = outpath
        else:
            self._outpath = os.getcwd()
        
        self._reference_samples = reference_samples
        self._test_samples = test_samples

        self._overwrite=overwrite
        self._min_coverage = min_coverage
        self._max_coverage = downsample_high_coverage
        self._min_ref_length = min_ref_length
        self._reference_fasta = fasta_fn
        self._bed_fn = bed_fn
        self._comparison_methods = comparison_methods
        self._logit = logit
        self._anova = anova
        self._allow_warnings = allow_warnings
        self._sequence_context = sequence_context
        self._sequence_context_weights = sequence_context_weights
        self._nthreads = nthreads - 1
        self._progress = progress
        self._select_ref_ids = select_ref_id
        self._exclude_ref_ids = exclude_ref_id

        # Get number of samples
        self._n_samples = len(self._reference_samples) + len(self._test_samples)

        self._valid_transcripts = self._getValidTranscripts()

    def __call__(self):
        """
        Run the analysis
        """
        logger.info("Starting data processing")

        in_q = mp.Queue(maxsize = 100)
        out_q = mp.Queue(maxsize = 100)
        error_q = mp.Queue(maxsize = 100)

        for tx in self._valid_transcripts:
            in_q.put(tx)

        self.resultsManager = SampCompResultsmanager.resultsManager(outpath=self._outpath, 
                                                                    prefix=self._outprefix, 
                                                                    overwrite=self._overwrite,
                                                                    bed_annotation=self._bed_fn,
                                                                    correction_method='fdr_bh')

        self.txComp = TxComp.TxComp(num_reference_samples = len(self._reference_samples),
                                    num_test_samples = len(self._test_samples), 
                                    random_state = 26,
                                    methods=self._comparison_methods,
                                    sequence_context=self._sequence_context,
                                    min_coverage=self._min_coverage,
                                    sequence_context_weights=self._sequence_context_weights,
                                    anova=self._anova,
                                    logit=self._logit,
                                    allow_warnings=self._allow_warnings)
        # Define processes
        processes = list()
        processes.append(mp.Process(target=self._list_refid, args=(in_q, error_q)))
        processes.append(mp.Process(target=self._writeResults, args=(out_q, error_q)))
        for i in range(self._nthreads):
            in_q.put(None)
            processes.append(mp.Process(target=self._processTx, args=(in_q, out_q, error_q)))

        
        try:
            #Start all processes
            for p in processes:
                p.start()

            # Monitor error queue
            for tb in iter(error_q.get, None):
                logger.trace("Error caught from error_q")
                raise NanocomporeError(tb)
            logger.debug("Error queue was closed")
            
            # Soft processes and queues stopping
            logger.debug("Waiting for all processes to be joined")
            for p in processes:
                p.join()
            logger.debug("All processes joined successfully")

            logger.debug("Closing all queues")
            for q in (in_q, out_q, error_q):
                q.close()
            logger.debug("All queues were closed")

            self.resultsManager.finish()

        except Exception as E:
            logger.error("An error occured. Killing all processes and closing queues\n")
            try:
                for p in processes:
                    p.terminate()
                for q in (in_q, out_q, error_q):
                    q.close()
            except:
                logger.error("An error occured while trying to kill processes\n")
            raise E
        finally:
            self.resultsManager.closeDB()


    #~~~~~~~~~~~~~~PRIVATE MULTIPROCESSING METHOD~~~~~~~~~~~~~~#
    def _processTx(self, in_q, out_q, error_q):
        logger.debug("Worker thread started")
        try:
            n_tx = 0
            for txid in iter(in_q.get, None):
                logger.debug(f"Worker thread processing new item from in_q: {txid}")
                tx_length = self._txid2length[txid]
                transcript = TranscriptObject.Transcript_Data(txid, self._reference_samples, self._test_samples, tx_length, self._max_coverage, self._min_coverage)
                if transcript.enoughTxCoverage(self._min_coverage):
                    n_tx += 1
                    logger.debug(f"Collecting data for {txid}")
                    results = self.txComp.txCompare(transcript)
                    out_q.put((transcript.name, results))
                else:
                    logger.debug(f"Insufficent coverage for {txid} skipping transcript")
                transcript.closeAllDbs()

        except:
            logger.error("Error in Worker")
            error_q.put(NanocomporeError(traceback.format_exc()))

        # Deal poison pill and close file pointer
        finally:
            logger.debug(f"Processed Transcrits: {n_tx}")
            logger.debug("Adding poison pill to out_q")
            out_q.put(None)

    def _writeResults(self, out_q, error_q):
        #TODO need to determine if this is necessary
        #self.resultsManager.saveExperimentMetadata()

        try:
            n_tx = 0
            n_pos = 0
            for _ in range(self._nthreads):
                for tx, result in iter(out_q.get, None):
                    if result:
                        logger.debug(f"Writer thread adding results data from {tx}")
                        n_tx += 1
                        n_pos = len([x for x in result if type(x) == int])
                        self.resultsManager.saveData(tx, result)
            self.resultsManager.finish()
        except:
            logger.error("Error writing results to database")
            error_q.put(NanocomporeError(traceback.format_exc()))
        finally:
            logger.debug("Written Transcripts:{} Valid positions:{}".format(n_tx, n_pos))
            self.resultsManager.closeDB()
            logger.info ("All Done. Transcripts processed: {}".format(n_tx))
            # Kill error queue with poison pill
            error_q.put(None)

    def _getValidTranscripts(self):
        self._txid2length = collections.defaultdict(int)
        validTranscripts = set()

        if self._select_ref_ids == self._exclude_ref_ids and (self._select_ref_ids or self._exclude_ref_ids):
            raise NanocomporeError("All of the gene ids to be selected are the same as the ids to be exluded")
        else:
            for record in SeqIO.parse(self._reference_fasta, "fasta"):
                transcript = record.id
                if len(record.seq) >= self._min_ref_length:
                    validTranscripts.add(transcript)
                    self._txid2length[transcript] = len(record.seq)

            if self._select_ref_ids:
                validTranscripts = validTranscripts.intersection(set(self._select_ref_ids))

            if self._exclude_ref_ids:
                validTranscripts = validTranscripts.difference(set(self._exclude_ref_ids))
        
        if validTranscripts:
            return list(validTranscripts)
        else:
            raise NanocomporeError("There are no valid transcripts")
    
    def _list_refid(self, in_q, error_q):
        """Add valid refid from whitelist to input queue to dispatch the data among the workers"""
        n_tx = 0
        try:
            for tx in self._valid_transcripts:
                logger.debug(f"Adding {tx} to in_q")
                in_q.put((tx))
                n_tx+=1

        # Manage exceptions and add error trackback to error queue
        except Exception:
            logger.debug("Error in Reader")
            error_q.put(traceback.format_exc())

        # Deal poison pills
        finally:
            for i in range (self._nthreads):
                in_q.put(None)
            logger.debug(f"Parsed transcripts:{n_tx}")