# -*- coding: utf-8 -*-

#~~~~~~~~~~~~~~IMPORTS~~~~~~~~~~~~~~#
# Std lib
from collections import *
import collections
import shelve
import multiprocessing as mp
import traceback
import datetime
import os

# Third party
from loguru import logger
import yaml
from tqdm import tqdm
import numpy as np
from pyfaidx import Fasta

# Local package
from nanocompore.common import *
from nanocompore.Whitelist import Whitelist
from nanocompore.TxComp import txCompare
from nanocompore.SampCompDB import SampCompDB
import nanocompore as pkg

# Disable multithreading for MKL and openBlas
os.environ["MKL_NUM_THREADS"] = "1"
os.environ["MKL_THREADING_LAYER"] = "sequential"
os.environ["NUMEXPR_NUM_THREADS"] = "1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ['OPENBLAS_NUM_THREADS'] = '1'

#~~~~~~~~~~~~~~MAIN CLASS~~~~~~~~~~~~~~#
class SampComp(object):
    """ Init analysis and check args"""

    #~~~~~~~~~~~~~~FUNDAMENTAL METHODS~~~~~~~~~~~~~~#

    def __init__(self,
        eventalign_fn_dict:dict,
        fasta_fn:str,
        bed_fn:str = None,
        outpath:str = "results",
        outprefix:str = "out_",
        overwrite:bool = False,
        whitelist:Whitelist = None,
        comparison_methods:list = ["GMM", "KS"],
        logit:bool = True,
        anova:bool = False,
        allow_warnings:bool = False,
        sequence_context:int = 0,
        sequence_context_weights:str = "uniform",
        min_coverage:int = 30,
        min_ref_length:int = 100,
        downsample_high_coverage:int = 5000,
        max_invalid_kmers_freq:float = 0.1,
        select_ref_id:list = [],
        exclude_ref_id:list = [],
        nthreads:int = 3,
        progress:bool = False):

        """
        Initialise a `SampComp` object and generates a white list of references with sufficient coverage for subsequent analysis.
        The retuned object can then be called to start the analysis.
        * eventalign_fn_dict
            Multilevel dictionnary indicating the condition_label, sample_label and file name of the eventalign_collapse output.
            2 conditions are expected and at least 2 sample replicates per condition are highly recommended.
            One can also pass YAML file describing the samples instead.
            Example `d = {"S1": {"R1":"path1.tsv", "R2":"path2.tsv"}, "S2": {"R1":"path3.tsv", "R2":"path4.tsv"}}`
        * outpath
            Path to the output folder.
        * outprefix
            text outprefix for all the files generated by the function.
        * overwrite
            If the output directory already exists, the standard behaviour is to raise an error to prevent overwriting existing data
            This option ignore the error and overwrite data if they have the same outpath and outprefix.
        * fasta_fn
            Path to a fasta file corresponding to the reference used for read alignment.
        * bed_fn
            Path to a BED file containing the annotation of the transcriptome used as reference when mapping.
        * whitelist
            Whitelist object previously generated with nanocompore Whitelist. If not given, will be automatically generated.
        * comparison_methods
            Statistical method to compare the 2 samples (mann_whitney or MW, kolmogorov_smirnov or KS, t_test or TT, gaussian_mixture_model or GMM).
            This can be a list or a comma separated string. {MW,KS,TT,GMM}
        * logit
            Force logistic regression even if we have less than 2 replicates in any condition.
        * allow_warnings
            If True runtime warnings during the ANOVA tests don't raise an error.
        * sequence_context
            Extend statistical analysis to contigous adjacent base if available.
        * sequence_context_weights
            type of weights to used for combining p-values. {uniform,harmonic}
        * min_coverage
            minimal read coverage required in all sample.
        * min_ref_length
            minimal length of a reference transcript to be considered in the analysis
        * downsample_high_coverage
            For reference with higher coverage, downsample by randomly selecting reads.
        * max_invalid_kmers_freq
            maximum frequency of NNNNN, mismatching and missing kmers in reads.
        * select_ref_id
            if given, only reference ids in the list will be selected for the analysis.
        * exclude_ref_id
            if given, refid in the list will be excluded from the analysis.
        * nthreads
            Number of threads (two are used for reading and writing, all the others for parallel processing).
        * progress
            Display a progress bar during execution
        """
        logger.info("Checking and initialising SampComp")

        # Save init options in dict for later
        log_init_state(loc=locals())


        # Check if fasta and bed files exist
        if not access_file(fasta_fn):
            raise NanocomporeError("{} is not a valid FASTA file".format(fasta_fn))
        if bed_fn and not access_file(bed_fn):
            raise NanocomporeError("{} is not a valid BED file".format(bed_fn))

        # Check threads number
        if nthreads < 2:
            raise NanocomporeError("The minimum number of threads is 3")

        # Parse comparison methods
        if comparison_methods:
            if type(comparison_methods) == str:
                comparison_methods = comparison_methods.split(",")
            for i, method in enumerate(comparison_methods):
                method = method.upper()
                if method in ["MANN_WHITNEY", "MW"]:
                    comparison_methods[i]="MW"
                elif method in ["KOLMOGOROV_SMIRNOV", "KS"]:
                    comparison_methods[i]="KS"
                elif method in ["T_TEST", "TT"]:
                    comparison_methods[i]="TT"
                elif method in ["GAUSSIAN_MIXTURE_MODEL", "GMM"]:
                    comparison_methods[i]="GMM"
                else:
                    raise NanocomporeError("Invalid comparison method {}".format(method))


        # Set private args
        self.__reference_samples = reference_samples
        self.__test_samples = test_samples

        self.__min_coverage = min_coverage
        self.__max_coverage = max_coverage
        self.__min_ref_length = min_ref_length
        self.__reference_fasta = reference
        self.__bed_fn = bed_fn
        self.__comparison_methods = comparison_methods
        self.__logit = logit
        self.__anova = anova
        self.__allow_warnings = allow_warnings
        self.__sequence_context = sequence_context
        self.__sequence_context_weights = sequence_context_weights
        self.__nthreads = nthreads - 1
        self.__progress = progress

        self.resultsDBmanager = ResultsDBmanager(outpath)

        # Get number of samples
        self.__n_samples = len(self.__reference_samples) + len(self.__test_samples)

    def __call__(self):
        """
        Run the analysis
        """
        logger.info("Starting data processing")

        valid_transcripts = self.__getValidTranscripts()
        in_q = valid_transcripts
        out_q = list()
        error_q = list()

        # Define processes
        processes = list()
        for (i in range(self.__nthreads)):
            in_q.put(null)
            processes.append(mp.Process(target=self.__processTx, args=(in_q, out_q, error_q)))

        writerProcess = mp.Process(target=self.__writeResults, args=(out_q, error_q))

        try:
            writerProcess.start()

            for p in processes:
                p.start()

            # Monitor error queue
            for tb in iter(error_q.get, None):
                logger.trace("Error caught from error_q")
                raise NanocomporeError(tb)
            
            for p in processes:
                p.join()

            out_q.put(null)
            writerProcess.join()

            for q in (in_q, out_q, error_q):
                q.close()
            
            self.__finish()

        except Exception as E:
            logger.error("An error occured. Killing all processes and closing queues\n")
            try:
                for ps in processes:
                    ps.terminate()
                writerProcess.terminate()
                for q in (in_q, out_q, error_q):
                    q.close()
            except:
                logger.error("An error occured while trying to kill processes\n")
            raise E


    #~~~~~~~~~~~~~~PRIVATE MULTIPROCESSING METHOD~~~~~~~~~~~~~~#
    def __processTx(self, in_q, out_q, error_q):
        while(txid in in_q.poll()):
            tx_length = self.__transcript2length[txid]
            transcript = Transcript_Data(txid, self.__reference_samples, self.__test_samples, tx_length)
            if transcript.enoughTxCoverage(self.__min_coverage):
                results = self.txComp(transcript)
                out_q.put(results)
            out_q.put(null)

    def __writeResults(self, out_q, error_q):
        self.resultsDBmanager.saveExperimentMetadata()

        while(res = out_q.poll()):
            self.resultsDBmanager.save(res)

        self.resultsDBmanager.finish()

    def __getValidTranscripts(self):
        self.__txid2length = collections.defaultdict(int)
        validTranscripts = []

        for record in SeqIO.parse(self.reference, "fasta"):
            transcript = record.id
            if len(record.seq) >= self.min_ref_length:
                validTranscripts.append(transcript)

        return validTranscripts

    
    def __process_references(self, in_q, out_q, error_q):
        """
        Consume ref_id, agregate intensity and dwell time at position level and
        perform statistical analyses to find significantly different regions
        """
        print_me = 0
        n_tx = n_reads = n_lines = 0
        try:
            logger.debug("Worker thread started")
            # Open all files for reading. File pointer are stored in a dict matching the ref_dict entries
            fp_dict = self.__eventalign_fn_open()

            # Process refid in input queue
            for ref_id, ref_dict in iter(in_q.get, None):
                logger.debug("Worker thread processing new item from in_q: {}".format(ref_id))
                # Create an empty dict for all positions first
                ref_pos_list = self.__make_ref_pos_list(ref_id)

                for cond_lab, sample_dict in ref_dict.items():
                    for sample_lab, read_list in sample_dict.items():
                        fp = fp_dict[cond_lab][sample_lab]

                        for read in read_list:

                            # Move to read, save read data chunk and reset file pointer
                            fp.seek(read["byte_offset"])
                            line_list = fp.read(read["byte_len"]).split("\n")
                            fp.seek(0)

                            # Check read_id ref_id concordance between index and data file
                            header = numeric_cast_list(line_list[0][1:].split("\t"))
                            if not header[0] == read["read_id"] or not header[1] == read["ref_id"]:
                                raise NanocomporeError("Index and data files are not matching:\n{}\n{}".format(header, read))

                            # Extract col names from second line
                            col_names = line_list[1].split("\t")
                            # Check that all required fields are present
                            if not all_values_in (["ref_pos", "ref_kmer", "median", "dwell_time", "variance", "prev_dwell"], col_names):
                                print("An error occured")
                                print(col_names, line_list[0])
                                raise NanocomporeError("Required fields not found in the data file: {}".format(col_names))
                            # Verify if kmers events stats values are present or not
                            kmers_stats = all_values_in (["NNNNN_dwell_time", "mismatch_dwell_time"], col_names)

                            # Parse data files kmers per kmers
                            prev_pos = None
                            for line in line_list[2:]:
                                # Transform line to dict and cast str numbers to actual numbers
                                kmer = numeric_cast_dict(keys=col_names, values=line.split("\t"))
                                pos = kmer["ref_pos"]

                                # Check consistance between eventalign data and reference sequence
                                if kmer["ref_kmer"] != ref_pos_list[pos]["ref_kmer"]:
                                    ref_pos_list[pos]["ref_kmer"] = ref_pos_list[pos]["ref_kmer"]+"!!!!"
                                    #raise NanocomporeError ("Data reference kmer({}) doesn't correspond to the reference sequence ({})".format(ref_pos_list[pos]["ref_kmer"], kmer["ref_kmer"]))

                                # Fill dict with the current pos values
                                #print(kmer["ref_pos"], kmer["ref_kmer"], read["read_id"], read["ref_id"], kmer["median"])
                                ref_pos_list[pos]["data"][cond_lab][sample_lab]["intensity"].append(kmer["median"])
                                ref_pos_list[pos]["data"][cond_lab][sample_lab]["dwell"].append(kmer["dwell_time"])
                                ref_pos_list[pos]["data"][cond_lab][sample_lab]["coverage"] += 1
                                #logan adding vairnce here
                                ref_pos_list[pos]["data"][cond_lab][sample_lab]["variance"].append(kmer["variance"])
                                ref_pos_list[pos]["data"][cond_lab][sample_lab]["prev_dwell"].append(kmer["prev_dwell"])

                                if kmers_stats:
                                    # Fill in the missing positions
                                    if prev_pos and pos-prev_pos > 1:
                                        for missing_pos in range(prev_pos+1, pos):
                                            ref_pos_list[missing_pos]["data"][cond_lab][sample_lab]["kmers_stats"]["missing"] += 1
                                    # Also fill in with normalised position event stats
                                    n_valid = (kmer["dwell_time"]-(kmer["NNNNN_dwell_time"]+kmer["mismatch_dwell_time"])) / kmer["dwell_time"]
                                    n_NNNNN = kmer["NNNNN_dwell_time"] / kmer["dwell_time"]
                                    n_mismatching = kmer["mismatch_dwell_time"] / kmer["dwell_time"]
                                    ref_pos_list[pos]["data"][cond_lab][sample_lab]["kmers_stats"]["valid"] += n_valid
                                    ref_pos_list[pos]["data"][cond_lab][sample_lab]["kmers_stats"]["NNNNN"] += n_NNNNN
                                    ref_pos_list[pos]["data"][cond_lab][sample_lab]["kmers_stats"]["mismatching"] += n_mismatching
                                    # Save previous position
                                    prev_pos = pos

                                n_lines+=1
                            n_reads+=1

                logger.debug("Data for {} loaded.".format(ref_id))
                if self.__comparison_methods:
                    random_state=np.random.RandomState(seed=42)
                    ref_pos_list = txCompare(
                        ref_id=ref_id,
                        ref_pos_list=ref_pos_list,
                        methods=self.__comparison_methods,
                        sequence_context=self.__sequence_context,
                        sequence_context_weights=self.__sequence_context_weights,
                        min_coverage= self.__min_coverage,
                        allow_warnings=self.__allow_warnings,
                        logit=self.__logit,
                        anova=self.__anova,
                        random_state=random_state)

                # Add the current read details to queue
                logger.debug("Adding %s to out_q"%(ref_id))
                out_q.put((ref_id, ref_pos_list))
                n_tx+=1

        # Manage exceptions and add error trackback to error queue
        except Exception as e:
            logger.error("Error in Worker")
            error_q.put (NanocomporeError(traceback.format_exc()))

        # Deal poison pill and close file pointer
        finally:
            logger.debug("Processed Transcrits:{} Reads:{} Lines:{}".format(n_tx, n_reads, n_lines))
            logger.debug("Adding poison pill to out_q")
            self.__eventalign_fn_close(fp_dict)
            out_q.put(None)
